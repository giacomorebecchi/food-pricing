pytorch:
  # Generic training settings
  max_epochs: 15
  batch_size: 32
  loader_workers: 8
  lazy_dataset: False
  shuffle_train_dataset: True
  num_sanity_val_steps: 0
  accumulate_grad_batches: null
  accelerator: auto
  devices: 1
  # Size of initial and intermediate embeddings
  img_dim: 224
  embedding_dim: 300
  language_feature_dim: 512
  vision_feature_dim: 512
  # Size of the layer after the concatenation 
  fusion_output_dim: 512
  # Dropout
  dropout_p: 0.3
  # Callbacks
  verbose: True
  early_stop_patience: 5
  backup_n_epochs: 5
  # Optimization settings
  optimizer_name: adamw
  optimizer_lr: 0.003
  optimizer_weight_decay: 0.01
  lr_scheduler_factor: 0.2
  lr_scheduler_patience: 2
  # Encoder optimization settings
  encoder_optimizer_name: radam
  encoder_optimizer_lr: 0.0001
  encoder_optimizer_weight_decay: 0
  encoder_lr_scheduler_factor: 0.1
  encoder_lr_scheduler_patience: 2
  # Storage of test predictions
  store_submission_frame: True
  # Unfreezing of encoders parameters
  n_epochs_unfreeze_language_module: 10
  n_epochs_unfreeze_vision_module: 8
  n_epochs_unfreeze_dual_module: 10

xgb:
  load_data: False
  num_round: 100
